{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FARE_CNT_1</th>\n",
       "      <th>FARE_CNT_2</th>\n",
       "      <th>FARE_CNT_3</th>\n",
       "      <th>FARE_CNT_4</th>\n",
       "      <th>FARE_CNT_5</th>\n",
       "      <th>FARE_CNT_6</th>\n",
       "      <th>FARE_CNT_7</th>\n",
       "      <th>FARE_CNT_8</th>\n",
       "      <th>FARE_CNT_9</th>\n",
       "      <th>FARE_CNT_10</th>\n",
       "      <th>FARE_CNT_11</th>\n",
       "      <th>FARE_CNT_12</th>\n",
       "      <th>FARE_CNT_13</th>\n",
       "      <th>FARE_CNT_14</th>\n",
       "      <th>FARE_AVG_1</th>\n",
       "      <th>FARE_AVG_2</th>\n",
       "      <th>FARE_AVG_3</th>\n",
       "      <th>FARE_AVG_4</th>\n",
       "      <th>FARE_AVG_5</th>\n",
       "      <th>FARE_AVG_6</th>\n",
       "      <th>FARE_AVG_7</th>\n",
       "      <th>FARE_AVG_8</th>\n",
       "      <th>FARE_AVG_9</th>\n",
       "      <th>FARE_AVG_10</th>\n",
       "      <th>FARE_AVG_11</th>\n",
       "      <th>FARE_AVG_12</th>\n",
       "      <th>FARE_AVG_13</th>\n",
       "      <th>FARE_AVG_14</th>\n",
       "      <th>FARE_MIN_1</th>\n",
       "      <th>FARE_MIN_2</th>\n",
       "      <th>FARE_MIN_3</th>\n",
       "      <th>FARE_MIN_4</th>\n",
       "      <th>FARE_MIN_5</th>\n",
       "      <th>FARE_MIN_6</th>\n",
       "      <th>FARE_MIN_7</th>\n",
       "      <th>FARE_MIN_8</th>\n",
       "      <th>FARE_MIN_9</th>\n",
       "      <th>FARE_MIN_10</th>\n",
       "      <th>FARE_MIN_11</th>\n",
       "      <th>FARE_MIN_12</th>\n",
       "      <th>FARE_MIN_13</th>\n",
       "      <th>FARE_MIN_14</th>\n",
       "      <th>FARE_MAX_1</th>\n",
       "      <th>FARE_MAX_2</th>\n",
       "      <th>FARE_MAX_3</th>\n",
       "      <th>FARE_MAX_4</th>\n",
       "      <th>FARE_MAX_5</th>\n",
       "      <th>FARE_MAX_6</th>\n",
       "      <th>FARE_MAX_7</th>\n",
       "      <th>FARE_MAX_8</th>\n",
       "      <th>FARE_MAX_9</th>\n",
       "      <th>FARE_MAX_10</th>\n",
       "      <th>FARE_MAX_11</th>\n",
       "      <th>FARE_MAX_12</th>\n",
       "      <th>FARE_MAX_13</th>\n",
       "      <th>FARE_MAX_14</th>\n",
       "      <th>DOW_0</th>\n",
       "      <th>DOW_1</th>\n",
       "      <th>DOW_2</th>\n",
       "      <th>DOW_3</th>\n",
       "      <th>DOW_4</th>\n",
       "      <th>DOW_5</th>\n",
       "      <th>DOW_6</th>\n",
       "      <th>HOLIDAY_N</th>\n",
       "      <th>HOLIDAY_Y</th>\n",
       "      <th>PSG_ARRIVE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPART_DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-15</th>\n",
       "      <td>261.0</td>\n",
       "      <td>277.00</td>\n",
       "      <td>286.00</td>\n",
       "      <td>298.00</td>\n",
       "      <td>304.00</td>\n",
       "      <td>494.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>308.00</td>\n",
       "      <td>309.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>99082.528736</td>\n",
       "      <td>95583.393502</td>\n",
       "      <td>93674.895105</td>\n",
       "      <td>88657.651007</td>\n",
       "      <td>85740.197368</td>\n",
       "      <td>77668.137652</td>\n",
       "      <td>76185.249501</td>\n",
       "      <td>82922.792208</td>\n",
       "      <td>81417.928803</td>\n",
       "      <td>80720.451613</td>\n",
       "      <td>81558.051948</td>\n",
       "      <td>82642.084691</td>\n",
       "      <td>82211.197411</td>\n",
       "      <td>81805.993485</td>\n",
       "      <td>35900.0</td>\n",
       "      <td>29900.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20900.0</td>\n",
       "      <td>14900.0</td>\n",
       "      <td>9400.0</td>\n",
       "      <td>16800.0</td>\n",
       "      <td>15900.0</td>\n",
       "      <td>15900.0</td>\n",
       "      <td>16900.0</td>\n",
       "      <td>17100.0</td>\n",
       "      <td>17100.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-16</th>\n",
       "      <td>276.0</td>\n",
       "      <td>284.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>297.00</td>\n",
       "      <td>310.00</td>\n",
       "      <td>310.00</td>\n",
       "      <td>519.00</td>\n",
       "      <td>520.00</td>\n",
       "      <td>313.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>93041.195652</td>\n",
       "      <td>88798.978873</td>\n",
       "      <td>87757.366548</td>\n",
       "      <td>84626.161616</td>\n",
       "      <td>78322.838710</td>\n",
       "      <td>75561.354839</td>\n",
       "      <td>67394.315992</td>\n",
       "      <td>65738.807692</td>\n",
       "      <td>71235.527157</td>\n",
       "      <td>69733.514377</td>\n",
       "      <td>68352.658228</td>\n",
       "      <td>69676.265823</td>\n",
       "      <td>69548.459119</td>\n",
       "      <td>68481.352201</td>\n",
       "      <td>58900.0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>29000.0</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>21900.0</td>\n",
       "      <td>16800.0</td>\n",
       "      <td>7900.0</td>\n",
       "      <td>16800.0</td>\n",
       "      <td>14900.0</td>\n",
       "      <td>16900.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>14900.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-17</th>\n",
       "      <td>146.0</td>\n",
       "      <td>146.00</td>\n",
       "      <td>175.00</td>\n",
       "      <td>209.00</td>\n",
       "      <td>236.00</td>\n",
       "      <td>257.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>460.00</td>\n",
       "      <td>468.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>75035.342466</td>\n",
       "      <td>75035.342466</td>\n",
       "      <td>71176.628571</td>\n",
       "      <td>74001.483254</td>\n",
       "      <td>73377.203390</td>\n",
       "      <td>69032.607004</td>\n",
       "      <td>66410.106762</td>\n",
       "      <td>60509.391304</td>\n",
       "      <td>59282.927350</td>\n",
       "      <td>62757.483221</td>\n",
       "      <td>61302.785235</td>\n",
       "      <td>60606.466667</td>\n",
       "      <td>60930.232558</td>\n",
       "      <td>60640.264901</td>\n",
       "      <td>25900.0</td>\n",
       "      <td>25900.0</td>\n",
       "      <td>21900.0</td>\n",
       "      <td>21900.0</td>\n",
       "      <td>15900.0</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>11900.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>5900.0</td>\n",
       "      <td>10900.0</td>\n",
       "      <td>8800.0</td>\n",
       "      <td>10900.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>11100.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-18</th>\n",
       "      <td>192.0</td>\n",
       "      <td>209.50</td>\n",
       "      <td>227.00</td>\n",
       "      <td>231.00</td>\n",
       "      <td>254.00</td>\n",
       "      <td>266.00</td>\n",
       "      <td>275.00</td>\n",
       "      <td>279.00</td>\n",
       "      <td>474.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>71417.552083</td>\n",
       "      <td>71102.344324</td>\n",
       "      <td>70787.136564</td>\n",
       "      <td>67056.580087</td>\n",
       "      <td>66701.811024</td>\n",
       "      <td>65017.706767</td>\n",
       "      <td>61368.072727</td>\n",
       "      <td>60719.892473</td>\n",
       "      <td>53512.236287</td>\n",
       "      <td>52664.957983</td>\n",
       "      <td>57113.687943</td>\n",
       "      <td>56271.943463</td>\n",
       "      <td>55610.457746</td>\n",
       "      <td>55057.429577</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>20500.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>14900.0</td>\n",
       "      <td>11900.0</td>\n",
       "      <td>10900.0</td>\n",
       "      <td>8700.0</td>\n",
       "      <td>7590.0</td>\n",
       "      <td>5900.0</td>\n",
       "      <td>5900.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>6900.0</td>\n",
       "      <td>6900.0</td>\n",
       "      <td>6900.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-19</th>\n",
       "      <td>203.0</td>\n",
       "      <td>244.00</td>\n",
       "      <td>254.00</td>\n",
       "      <td>264.00</td>\n",
       "      <td>268.00</td>\n",
       "      <td>275.00</td>\n",
       "      <td>282.00</td>\n",
       "      <td>284.00</td>\n",
       "      <td>285.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>68348.719212</td>\n",
       "      <td>64762.581967</td>\n",
       "      <td>64268.734165</td>\n",
       "      <td>63774.886364</td>\n",
       "      <td>61551.492537</td>\n",
       "      <td>60935.418182</td>\n",
       "      <td>60320.531915</td>\n",
       "      <td>56492.042254</td>\n",
       "      <td>55271.017544</td>\n",
       "      <td>49787.607362</td>\n",
       "      <td>49641.093117</td>\n",
       "      <td>54815.342466</td>\n",
       "      <td>54000.443686</td>\n",
       "      <td>53016.564626</td>\n",
       "      <td>24600.0</td>\n",
       "      <td>20010.0</td>\n",
       "      <td>19955.0</td>\n",
       "      <td>19900.0</td>\n",
       "      <td>17900.0</td>\n",
       "      <td>14500.0</td>\n",
       "      <td>12900.0</td>\n",
       "      <td>11300.0</td>\n",
       "      <td>9660.0</td>\n",
       "      <td>6900.0</td>\n",
       "      <td>6900.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>6210.0</td>\n",
       "      <td>6210.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>97.0</td>\n",
       "      <td>117.75</td>\n",
       "      <td>138.50</td>\n",
       "      <td>159.25</td>\n",
       "      <td>180.00</td>\n",
       "      <td>191.00</td>\n",
       "      <td>195.00</td>\n",
       "      <td>187.00</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>87592.783505</td>\n",
       "      <td>85522.518184</td>\n",
       "      <td>83452.252864</td>\n",
       "      <td>81381.987543</td>\n",
       "      <td>79311.722222</td>\n",
       "      <td>78666.387435</td>\n",
       "      <td>78692.256410</td>\n",
       "      <td>76474.919786</td>\n",
       "      <td>77888.650000</td>\n",
       "      <td>77221.100000</td>\n",
       "      <td>77585.099010</td>\n",
       "      <td>75680.878049</td>\n",
       "      <td>73006.367925</td>\n",
       "      <td>72395.280374</td>\n",
       "      <td>65900.0</td>\n",
       "      <td>61650.0</td>\n",
       "      <td>57400.0</td>\n",
       "      <td>53150.0</td>\n",
       "      <td>48900.0</td>\n",
       "      <td>43470.0</td>\n",
       "      <td>43000.0</td>\n",
       "      <td>41000.0</td>\n",
       "      <td>41000.0</td>\n",
       "      <td>40900.0</td>\n",
       "      <td>39000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>31900.0</td>\n",
       "      <td>28900.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>121.0</td>\n",
       "      <td>152.00</td>\n",
       "      <td>155.75</td>\n",
       "      <td>159.50</td>\n",
       "      <td>163.25</td>\n",
       "      <td>167.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>209.00</td>\n",
       "      <td>206.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>86455.371901</td>\n",
       "      <td>83598.684211</td>\n",
       "      <td>81637.126930</td>\n",
       "      <td>79675.569650</td>\n",
       "      <td>77714.012370</td>\n",
       "      <td>75752.455090</td>\n",
       "      <td>79295.294118</td>\n",
       "      <td>78679.952153</td>\n",
       "      <td>78315.825243</td>\n",
       "      <td>77942.585366</td>\n",
       "      <td>75999.653465</td>\n",
       "      <td>75599.951923</td>\n",
       "      <td>74911.542056</td>\n",
       "      <td>74544.803922</td>\n",
       "      <td>59760.0</td>\n",
       "      <td>55900.0</td>\n",
       "      <td>55900.0</td>\n",
       "      <td>55900.0</td>\n",
       "      <td>55900.0</td>\n",
       "      <td>55900.0</td>\n",
       "      <td>52900.0</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>50900.0</td>\n",
       "      <td>46230.0</td>\n",
       "      <td>44160.0</td>\n",
       "      <td>42900.0</td>\n",
       "      <td>41900.0</td>\n",
       "      <td>40020.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>99.0</td>\n",
       "      <td>125.00</td>\n",
       "      <td>140.00</td>\n",
       "      <td>143.25</td>\n",
       "      <td>146.50</td>\n",
       "      <td>149.75</td>\n",
       "      <td>153.00</td>\n",
       "      <td>158.00</td>\n",
       "      <td>158.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>80488.484848</td>\n",
       "      <td>77330.800000</td>\n",
       "      <td>76668.357143</td>\n",
       "      <td>77001.905112</td>\n",
       "      <td>77335.453081</td>\n",
       "      <td>77669.001050</td>\n",
       "      <td>78002.549020</td>\n",
       "      <td>78167.531646</td>\n",
       "      <td>79609.177215</td>\n",
       "      <td>79457.937500</td>\n",
       "      <td>81942.176471</td>\n",
       "      <td>80424.035088</td>\n",
       "      <td>80270.958084</td>\n",
       "      <td>81016.982249</td>\n",
       "      <td>59340.0</td>\n",
       "      <td>58650.0</td>\n",
       "      <td>58900.0</td>\n",
       "      <td>58395.0</td>\n",
       "      <td>57890.0</td>\n",
       "      <td>57385.0</td>\n",
       "      <td>56880.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>55900.0</td>\n",
       "      <td>55900.0</td>\n",
       "      <td>55900.0</td>\n",
       "      <td>57900.0</td>\n",
       "      <td>51900.0</td>\n",
       "      <td>51900.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>142000.0</td>\n",
       "      <td>143000.0</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>145000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>188.0</td>\n",
       "      <td>205.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>212.00</td>\n",
       "      <td>208.25</td>\n",
       "      <td>204.50</td>\n",
       "      <td>200.75</td>\n",
       "      <td>197.00</td>\n",
       "      <td>196.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>84146.914894</td>\n",
       "      <td>81221.463415</td>\n",
       "      <td>82157.000000</td>\n",
       "      <td>85402.641509</td>\n",
       "      <td>88017.970980</td>\n",
       "      <td>90633.300450</td>\n",
       "      <td>93248.629921</td>\n",
       "      <td>95863.959391</td>\n",
       "      <td>100622.653061</td>\n",
       "      <td>103738.256410</td>\n",
       "      <td>105956.391753</td>\n",
       "      <td>105849.393939</td>\n",
       "      <td>106776.345178</td>\n",
       "      <td>109349.435897</td>\n",
       "      <td>24380.0</td>\n",
       "      <td>24380.0</td>\n",
       "      <td>19900.0</td>\n",
       "      <td>33900.0</td>\n",
       "      <td>34400.0</td>\n",
       "      <td>34900.0</td>\n",
       "      <td>35400.0</td>\n",
       "      <td>35900.0</td>\n",
       "      <td>46640.0</td>\n",
       "      <td>56900.0</td>\n",
       "      <td>59900.0</td>\n",
       "      <td>57900.0</td>\n",
       "      <td>65900.0</td>\n",
       "      <td>71900.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31</th>\n",
       "      <td>245.0</td>\n",
       "      <td>259.00</td>\n",
       "      <td>266.00</td>\n",
       "      <td>156.00</td>\n",
       "      <td>169.00</td>\n",
       "      <td>187.75</td>\n",
       "      <td>206.50</td>\n",
       "      <td>225.25</td>\n",
       "      <td>244.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>75267.265306</td>\n",
       "      <td>75983.861004</td>\n",
       "      <td>75136.315789</td>\n",
       "      <td>67413.846154</td>\n",
       "      <td>78885.325444</td>\n",
       "      <td>81643.830148</td>\n",
       "      <td>84402.334853</td>\n",
       "      <td>87160.839558</td>\n",
       "      <td>89919.344262</td>\n",
       "      <td>90583.620690</td>\n",
       "      <td>98733.103448</td>\n",
       "      <td>99728.036530</td>\n",
       "      <td>101537.017544</td>\n",
       "      <td>101731.504425</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>4900.0</td>\n",
       "      <td>4900.0</td>\n",
       "      <td>5900.0</td>\n",
       "      <td>7900.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>12900.0</td>\n",
       "      <td>15400.0</td>\n",
       "      <td>17900.0</td>\n",
       "      <td>20140.0</td>\n",
       "      <td>34980.0</td>\n",
       "      <td>46900.0</td>\n",
       "      <td>46900.0</td>\n",
       "      <td>46900.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             FARE_CNT_1  FARE_CNT_2  FARE_CNT_3  FARE_CNT_4  FARE_CNT_5  \\\n",
       "DEPART_DATE                                                               \n",
       "2022-01-15        261.0      277.00      286.00      298.00      304.00   \n",
       "2022-01-16        276.0      284.00      281.00      297.00      310.00   \n",
       "2022-01-17        146.0      146.00      175.00      209.00      236.00   \n",
       "2022-01-18        192.0      209.50      227.00      231.00      254.00   \n",
       "2022-01-19        203.0      244.00      254.00      264.00      268.00   \n",
       "...                 ...         ...         ...         ...         ...   \n",
       "2022-12-27         97.0      117.75      138.50      159.25      180.00   \n",
       "2022-12-28        121.0      152.00      155.75      159.50      163.25   \n",
       "2022-12-29         99.0      125.00      140.00      143.25      146.50   \n",
       "2022-12-30        188.0      205.00      200.00      212.00      208.25   \n",
       "2022-12-31        245.0      259.00      266.00      156.00      169.00   \n",
       "\n",
       "             FARE_CNT_6  FARE_CNT_7  FARE_CNT_8  FARE_CNT_9  FARE_CNT_10  \\\n",
       "DEPART_DATE                                                                \n",
       "2022-01-15       494.00      501.00      308.00       309.0        310.0   \n",
       "2022-01-16       310.00      519.00      520.00       313.0        313.0   \n",
       "2022-01-17       257.00      281.00      460.00       468.0        298.0   \n",
       "2022-01-18       266.00      275.00      279.00       474.0        476.0   \n",
       "2022-01-19       275.00      282.00      284.00       285.0        489.0   \n",
       "...                 ...         ...         ...         ...          ...   \n",
       "2022-12-27       191.00      195.00      187.00       200.0        200.0   \n",
       "2022-12-28       167.00      204.00      209.00       206.0        205.0   \n",
       "2022-12-29       149.75      153.00      158.00       158.0        160.0   \n",
       "2022-12-30       204.50      200.75      197.00       196.0        195.0   \n",
       "2022-12-31       187.75      206.50      225.25       244.0        232.0   \n",
       "\n",
       "             FARE_CNT_11  FARE_CNT_12  FARE_CNT_13  FARE_CNT_14    FARE_AVG_1  \\\n",
       "DEPART_DATE                                                                     \n",
       "2022-01-15         308.0        307.0        309.0        307.0  99082.528736   \n",
       "2022-01-16         316.0        316.0        318.0        318.0  93041.195652   \n",
       "2022-01-17         298.0        300.0        301.0        302.0  75035.342466   \n",
       "2022-01-18         282.0        283.0        284.0        284.0  71417.552083   \n",
       "2022-01-19         494.0        292.0        293.0        294.0  68348.719212   \n",
       "...                  ...          ...          ...          ...           ...   \n",
       "2022-12-27         202.0        205.0        212.0        214.0  87592.783505   \n",
       "2022-12-28         202.0        208.0        214.0        204.0  86455.371901   \n",
       "2022-12-29         170.0        171.0        167.0        169.0  80488.484848   \n",
       "2022-12-30         194.0        198.0        197.0        195.0  84146.914894   \n",
       "2022-12-31         232.0        219.0        228.0        226.0  75267.265306   \n",
       "\n",
       "               FARE_AVG_2    FARE_AVG_3    FARE_AVG_4    FARE_AVG_5  \\\n",
       "DEPART_DATE                                                           \n",
       "2022-01-15   95583.393502  93674.895105  88657.651007  85740.197368   \n",
       "2022-01-16   88798.978873  87757.366548  84626.161616  78322.838710   \n",
       "2022-01-17   75035.342466  71176.628571  74001.483254  73377.203390   \n",
       "2022-01-18   71102.344324  70787.136564  67056.580087  66701.811024   \n",
       "2022-01-19   64762.581967  64268.734165  63774.886364  61551.492537   \n",
       "...                   ...           ...           ...           ...   \n",
       "2022-12-27   85522.518184  83452.252864  81381.987543  79311.722222   \n",
       "2022-12-28   83598.684211  81637.126930  79675.569650  77714.012370   \n",
       "2022-12-29   77330.800000  76668.357143  77001.905112  77335.453081   \n",
       "2022-12-30   81221.463415  82157.000000  85402.641509  88017.970980   \n",
       "2022-12-31   75983.861004  75136.315789  67413.846154  78885.325444   \n",
       "\n",
       "               FARE_AVG_6    FARE_AVG_7    FARE_AVG_8     FARE_AVG_9  \\\n",
       "DEPART_DATE                                                            \n",
       "2022-01-15   77668.137652  76185.249501  82922.792208   81417.928803   \n",
       "2022-01-16   75561.354839  67394.315992  65738.807692   71235.527157   \n",
       "2022-01-17   69032.607004  66410.106762  60509.391304   59282.927350   \n",
       "2022-01-18   65017.706767  61368.072727  60719.892473   53512.236287   \n",
       "2022-01-19   60935.418182  60320.531915  56492.042254   55271.017544   \n",
       "...                   ...           ...           ...            ...   \n",
       "2022-12-27   78666.387435  78692.256410  76474.919786   77888.650000   \n",
       "2022-12-28   75752.455090  79295.294118  78679.952153   78315.825243   \n",
       "2022-12-29   77669.001050  78002.549020  78167.531646   79609.177215   \n",
       "2022-12-30   90633.300450  93248.629921  95863.959391  100622.653061   \n",
       "2022-12-31   81643.830148  84402.334853  87160.839558   89919.344262   \n",
       "\n",
       "               FARE_AVG_10    FARE_AVG_11    FARE_AVG_12    FARE_AVG_13  \\\n",
       "DEPART_DATE                                                               \n",
       "2022-01-15    80720.451613   81558.051948   82642.084691   82211.197411   \n",
       "2022-01-16    69733.514377   68352.658228   69676.265823   69548.459119   \n",
       "2022-01-17    62757.483221   61302.785235   60606.466667   60930.232558   \n",
       "2022-01-18    52664.957983   57113.687943   56271.943463   55610.457746   \n",
       "2022-01-19    49787.607362   49641.093117   54815.342466   54000.443686   \n",
       "...                    ...            ...            ...            ...   \n",
       "2022-12-27    77221.100000   77585.099010   75680.878049   73006.367925   \n",
       "2022-12-28    77942.585366   75999.653465   75599.951923   74911.542056   \n",
       "2022-12-29    79457.937500   81942.176471   80424.035088   80270.958084   \n",
       "2022-12-30   103738.256410  105956.391753  105849.393939  106776.345178   \n",
       "2022-12-31    90583.620690   98733.103448   99728.036530  101537.017544   \n",
       "\n",
       "               FARE_AVG_14  FARE_MIN_1  FARE_MIN_2  FARE_MIN_3  FARE_MIN_4  \\\n",
       "DEPART_DATE                                                                  \n",
       "2022-01-15    81805.993485     35900.0     29900.0     22000.0     20000.0   \n",
       "2022-01-16    68481.352201     58900.0     50000.0     45000.0     29000.0   \n",
       "2022-01-17    60640.264901     25900.0     25900.0     21900.0     21900.0   \n",
       "2022-01-18    55057.429577     22000.0     20500.0     19000.0     14900.0   \n",
       "2022-01-19    53016.564626     24600.0     20010.0     19955.0     19900.0   \n",
       "...                    ...         ...         ...         ...         ...   \n",
       "2022-12-27    72395.280374     65900.0     61650.0     57400.0     53150.0   \n",
       "2022-12-28    74544.803922     59760.0     55900.0     55900.0     55900.0   \n",
       "2022-12-29    81016.982249     59340.0     58650.0     58900.0     58395.0   \n",
       "2022-12-30   109349.435897     24380.0     24380.0     19900.0     33900.0   \n",
       "2022-12-31   101731.504425      4500.0      4900.0      4900.0      5900.0   \n",
       "\n",
       "             FARE_MIN_5  FARE_MIN_6  FARE_MIN_7  FARE_MIN_8  FARE_MIN_9  \\\n",
       "DEPART_DATE                                                               \n",
       "2022-01-15      20900.0     14900.0      9400.0     16800.0     15900.0   \n",
       "2022-01-16      23900.0     21900.0     16800.0      7900.0     16800.0   \n",
       "2022-01-17      15900.0     12800.0     11900.0     10400.0      5900.0   \n",
       "2022-01-18      11900.0     10900.0      8700.0      7590.0      5900.0   \n",
       "2022-01-19      17900.0     14500.0     12900.0     11300.0      9660.0   \n",
       "...                 ...         ...         ...         ...         ...   \n",
       "2022-12-27      48900.0     43470.0     43000.0     41000.0     41000.0   \n",
       "2022-12-28      55900.0     55900.0     52900.0     52000.0     50900.0   \n",
       "2022-12-29      57890.0     57385.0     56880.0     54000.0     55900.0   \n",
       "2022-12-30      34400.0     34900.0     35400.0     35900.0     46640.0   \n",
       "2022-12-31       7900.0     10400.0     12900.0     15400.0     17900.0   \n",
       "\n",
       "             FARE_MIN_10  FARE_MIN_11  FARE_MIN_12  FARE_MIN_13  FARE_MIN_14  \\\n",
       "DEPART_DATE                                                                    \n",
       "2022-01-15       15900.0      16900.0      17100.0      17100.0      14400.0   \n",
       "2022-01-16       14900.0      16900.0      14400.0      14900.0      14400.0   \n",
       "2022-01-17       10900.0       8800.0      10900.0       9600.0      11100.0   \n",
       "2022-01-18        5900.0       5600.0       6900.0       6900.0       6900.0   \n",
       "2022-01-19        6900.0       6900.0       7300.0       6210.0       6210.0   \n",
       "...                  ...          ...          ...          ...          ...   \n",
       "2022-12-27       40900.0      39000.0      35000.0      31900.0      28900.0   \n",
       "2022-12-28       46230.0      44160.0      42900.0      41900.0      40020.0   \n",
       "2022-12-29       55900.0      55900.0      57900.0      51900.0      51900.0   \n",
       "2022-12-30       56900.0      59900.0      57900.0      65900.0      71900.0   \n",
       "2022-12-31       20140.0      34980.0      46900.0      46900.0      46900.0   \n",
       "\n",
       "             FARE_MAX_1  FARE_MAX_2  FARE_MAX_3  FARE_MAX_4  FARE_MAX_5  \\\n",
       "DEPART_DATE                                                               \n",
       "2022-01-15     177000.0    177000.0    177000.0    177000.0    177000.0   \n",
       "2022-01-16     160000.0    160000.0    160000.0    160000.0    160000.0   \n",
       "2022-01-17     146000.0    146000.0    146000.0    146000.0    146000.0   \n",
       "2022-01-18     146000.0    146000.0    146000.0    146000.0    146000.0   \n",
       "2022-01-19     146000.0    146000.0    146000.0    146000.0    146000.0   \n",
       "...                 ...         ...         ...         ...         ...   \n",
       "2022-12-27     146000.0    146000.0    146000.0    146000.0    146000.0   \n",
       "2022-12-28     146000.0    146000.0    146000.0    146000.0    146000.0   \n",
       "2022-12-29     146000.0    146000.0    142000.0    143000.0    144000.0   \n",
       "2022-12-30     177000.0    177000.0    177000.0    177000.0    177000.0   \n",
       "2022-12-31     177000.0    177000.0    177000.0    177000.0    177000.0   \n",
       "\n",
       "             FARE_MAX_6  FARE_MAX_7  FARE_MAX_8  FARE_MAX_9  FARE_MAX_10  \\\n",
       "DEPART_DATE                                                                \n",
       "2022-01-15     177000.0    177000.0    177000.0    177000.0     177000.0   \n",
       "2022-01-16     160000.0    160000.0    160000.0    160000.0     160000.0   \n",
       "2022-01-17     146000.0    146000.0    146000.0    146000.0     146000.0   \n",
       "2022-01-18     146000.0    146000.0    146000.0    146000.0     146000.0   \n",
       "2022-01-19     146000.0    146000.0    146000.0    146000.0     146000.0   \n",
       "...                 ...         ...         ...         ...          ...   \n",
       "2022-12-27     146000.0    146000.0    146000.0    146000.0     146000.0   \n",
       "2022-12-28     146000.0    146000.0    146000.0    146000.0     146000.0   \n",
       "2022-12-29     145000.0    146000.0    146000.0    146000.0     146000.0   \n",
       "2022-12-30     177000.0    177000.0    177000.0    177000.0     177000.0   \n",
       "2022-12-31     177000.0    177000.0    177000.0    177000.0     177000.0   \n",
       "\n",
       "             FARE_MAX_11  FARE_MAX_12  FARE_MAX_13  FARE_MAX_14  DOW_0  DOW_1  \\\n",
       "DEPART_DATE                                                                     \n",
       "2022-01-15      177000.0     177000.0     177000.0     177000.0      0      0   \n",
       "2022-01-16      160000.0     160000.0     160000.0     160000.0      0      0   \n",
       "2022-01-17      146000.0     146000.0     146000.0     146000.0      1      0   \n",
       "2022-01-18      146000.0     146000.0     146000.0     146000.0      0      1   \n",
       "2022-01-19      146000.0     146000.0     146000.0     146000.0      0      0   \n",
       "...                  ...          ...          ...          ...    ...    ...   \n",
       "2022-12-27      146000.0     146000.0     146000.0     146000.0      0      1   \n",
       "2022-12-28      146000.0     146000.0     146000.0     146000.0      0      0   \n",
       "2022-12-29      146000.0     146000.0     146000.0     146000.0      0      0   \n",
       "2022-12-30      177000.0     177000.0     177000.0     177000.0      0      0   \n",
       "2022-12-31      177000.0     177000.0     177000.0     177000.0      0      0   \n",
       "\n",
       "             DOW_2  DOW_3  DOW_4  DOW_5  DOW_6  HOLIDAY_N  HOLIDAY_Y  \\\n",
       "DEPART_DATE                                                            \n",
       "2022-01-15       0      0      0      1      0          1          0   \n",
       "2022-01-16       0      0      0      0      1          1          0   \n",
       "2022-01-17       0      0      0      0      0          1          0   \n",
       "2022-01-18       0      0      0      0      0          1          0   \n",
       "2022-01-19       1      0      0      0      0          1          0   \n",
       "...            ...    ...    ...    ...    ...        ...        ...   \n",
       "2022-12-27       0      0      0      0      0          1          0   \n",
       "2022-12-28       1      0      0      0      0          1          0   \n",
       "2022-12-29       0      1      0      0      0          1          0   \n",
       "2022-12-30       0      0      1      0      0          1          0   \n",
       "2022-12-31       0      0      0      1      0          1          0   \n",
       "\n",
       "             PSG_ARRIVE  \n",
       "DEPART_DATE              \n",
       "2022-01-15        40794  \n",
       "2022-01-16        42494  \n",
       "2022-01-17        42803  \n",
       "2022-01-18        39383  \n",
       "2022-01-19        39045  \n",
       "...                 ...  \n",
       "2022-12-27        39755  \n",
       "2022-12-28        40709  \n",
       "2022-12-29        40401  \n",
       "2022-12-30        39568  \n",
       "2022-12-31        35319  \n",
       "\n",
       "[351 rows x 66 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./scaled_dataset.csv\")\n",
    "dataset = dataset.set_index(\"DEPART_DATE\")\n",
    "dataset = dataset.drop([\"AIR_ARRIVE\"], axis = 1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FARE_CNT_1</th>\n",
       "      <th>FARE_CNT_2</th>\n",
       "      <th>FARE_CNT_3</th>\n",
       "      <th>FARE_CNT_4</th>\n",
       "      <th>FARE_CNT_5</th>\n",
       "      <th>FARE_CNT_6</th>\n",
       "      <th>FARE_CNT_7</th>\n",
       "      <th>FARE_CNT_8</th>\n",
       "      <th>FARE_CNT_9</th>\n",
       "      <th>FARE_CNT_10</th>\n",
       "      <th>FARE_CNT_11</th>\n",
       "      <th>FARE_CNT_12</th>\n",
       "      <th>FARE_CNT_13</th>\n",
       "      <th>FARE_CNT_14</th>\n",
       "      <th>FARE_AVG_1</th>\n",
       "      <th>FARE_AVG_2</th>\n",
       "      <th>FARE_AVG_3</th>\n",
       "      <th>FARE_AVG_4</th>\n",
       "      <th>FARE_AVG_5</th>\n",
       "      <th>FARE_AVG_6</th>\n",
       "      <th>FARE_AVG_7</th>\n",
       "      <th>FARE_AVG_8</th>\n",
       "      <th>FARE_AVG_9</th>\n",
       "      <th>FARE_AVG_10</th>\n",
       "      <th>FARE_AVG_11</th>\n",
       "      <th>FARE_AVG_12</th>\n",
       "      <th>FARE_AVG_13</th>\n",
       "      <th>FARE_AVG_14</th>\n",
       "      <th>FARE_MIN_1</th>\n",
       "      <th>FARE_MIN_2</th>\n",
       "      <th>FARE_MIN_3</th>\n",
       "      <th>FARE_MIN_4</th>\n",
       "      <th>FARE_MIN_5</th>\n",
       "      <th>FARE_MIN_6</th>\n",
       "      <th>FARE_MIN_7</th>\n",
       "      <th>FARE_MIN_8</th>\n",
       "      <th>FARE_MIN_9</th>\n",
       "      <th>FARE_MIN_10</th>\n",
       "      <th>FARE_MIN_11</th>\n",
       "      <th>FARE_MIN_12</th>\n",
       "      <th>FARE_MIN_13</th>\n",
       "      <th>FARE_MIN_14</th>\n",
       "      <th>FARE_MAX_1</th>\n",
       "      <th>FARE_MAX_2</th>\n",
       "      <th>FARE_MAX_3</th>\n",
       "      <th>FARE_MAX_4</th>\n",
       "      <th>FARE_MAX_5</th>\n",
       "      <th>FARE_MAX_6</th>\n",
       "      <th>FARE_MAX_7</th>\n",
       "      <th>FARE_MAX_8</th>\n",
       "      <th>FARE_MAX_9</th>\n",
       "      <th>FARE_MAX_10</th>\n",
       "      <th>FARE_MAX_11</th>\n",
       "      <th>FARE_MAX_12</th>\n",
       "      <th>FARE_MAX_13</th>\n",
       "      <th>FARE_MAX_14</th>\n",
       "      <th>DOW_0</th>\n",
       "      <th>DOW_1</th>\n",
       "      <th>DOW_2</th>\n",
       "      <th>DOW_3</th>\n",
       "      <th>DOW_4</th>\n",
       "      <th>DOW_5</th>\n",
       "      <th>DOW_6</th>\n",
       "      <th>HOLIDAY_N</th>\n",
       "      <th>HOLIDAY_Y</th>\n",
       "      <th>PSG_ARRIVE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPART_DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [FARE_CNT_1, FARE_CNT_2, FARE_CNT_3, FARE_CNT_4, FARE_CNT_5, FARE_CNT_6, FARE_CNT_7, FARE_CNT_8, FARE_CNT_9, FARE_CNT_10, FARE_CNT_11, FARE_CNT_12, FARE_CNT_13, FARE_CNT_14, FARE_AVG_1, FARE_AVG_2, FARE_AVG_3, FARE_AVG_4, FARE_AVG_5, FARE_AVG_6, FARE_AVG_7, FARE_AVG_8, FARE_AVG_9, FARE_AVG_10, FARE_AVG_11, FARE_AVG_12, FARE_AVG_13, FARE_AVG_14, FARE_MIN_1, FARE_MIN_2, FARE_MIN_3, FARE_MIN_4, FARE_MIN_5, FARE_MIN_6, FARE_MIN_7, FARE_MIN_8, FARE_MIN_9, FARE_MIN_10, FARE_MIN_11, FARE_MIN_12, FARE_MIN_13, FARE_MIN_14, FARE_MAX_1, FARE_MAX_2, FARE_MAX_3, FARE_MAX_4, FARE_MAX_5, FARE_MAX_6, FARE_MAX_7, FARE_MAX_8, FARE_MAX_9, FARE_MAX_10, FARE_MAX_11, FARE_MAX_12, FARE_MAX_13, FARE_MAX_14, DOW_0, DOW_1, DOW_2, DOW_3, DOW_4, DOW_5, DOW_6, HOLIDAY_N, HOLIDAY_Y, PSG_ARRIVE]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nullê°’ í™•ì¸ \n",
    "dataset[dataset.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = dataset\n",
    "\n",
    "# # Drop the columns with missing values\n",
    "# df = df.dropna(axis=1)\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = df.drop(['PSG_ARRIVE'], axis=1)\n",
    "y = df['PSG_ARRIVE']\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "scaled_y = scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(scaled_X) * 0.8)\n",
    "test_size = len(scaled_X) - train_size\n",
    "train_X = scaled_X[0:200, :]\n",
    "test_X = scaled_X[200:len(scaled_X), :]\n",
    "train_y = scaled_y[0:200, :]\n",
    "test_y = scaled_y[200:len(scaled_y), :]\n",
    "\n",
    "# Define the function to create the dataset\n",
    "def create_dataset(X, y, look_back):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(X)-look_back-1):\n",
    "        a = X[i:(i+look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(y[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# Create the dataset\n",
    "look_back = 14\n",
    "trainX, trainY = create_dataset(train_X, train_y, look_back)\n",
    "testX, testY = create_dataset(test_X, test_y, look_back)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f797bb6e9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f797bb6e9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.1272WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f797ca80e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f797ca80e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "24/24 [==============================] - 3s 34ms/step - loss: 0.1115 - val_loss: 0.0248 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0137 - val_loss: 0.0231 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0124 - val_loss: 0.0242 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0101 - val_loss: 0.0229 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0090 - val_loss: 0.0247 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0079 - val_loss: 0.0229 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0316 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0262 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0239 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0072 - val_loss: 0.0240 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0272 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0273 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0273 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0279 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0231 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0231 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0265 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0308 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0244 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0227 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0331 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0255 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0301 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0274 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0281 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0237 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0270 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0271 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0283 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0229 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0241 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0238 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0286 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0268 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0265 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0347 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0268 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0257 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0237 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0283 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0264 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0313 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0240 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0230 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0259 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0235 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0272 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0259 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0255 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0301 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0251 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0252 - lr: 0.0010\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0255 - lr: 0.0010\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0235 - lr: 0.0010\n",
      "Epoch 55/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0298 - lr: 0.0010\n",
      "Epoch 56/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0252 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0246 - lr: 0.0010\n",
      "Epoch 58/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0287 - lr: 0.0010\n",
      "Epoch 59/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0247 - lr: 0.0010\n",
      "Epoch 60/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0235 - lr: 0.0010\n",
      "Epoch 61/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0274 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0272 - lr: 0.0010\n",
      "Epoch 63/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0216 - lr: 0.0010\n",
      "Epoch 64/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0247 - lr: 0.0010\n",
      "Epoch 65/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0255 - lr: 0.0010\n",
      "Epoch 66/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0270 - lr: 0.0010\n",
      "Epoch 67/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0335 - lr: 0.0010\n",
      "Epoch 68/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0296 - lr: 0.0010\n",
      "Epoch 69/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0306 - lr: 0.0010\n",
      "Epoch 70/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0253 - lr: 0.0010\n",
      "Epoch 71/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0289 - lr: 0.0010\n",
      "Epoch 72/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0219 - lr: 0.0010\n",
      "Epoch 73/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0242 - lr: 0.0010\n",
      "Epoch 74/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0252 - lr: 0.0010\n",
      "Epoch 75/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0257 - lr: 0.0010\n",
      "Epoch 76/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0270 - lr: 0.0010\n",
      "Epoch 77/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0248 - lr: 0.0010\n",
      "Epoch 78/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0240 - lr: 0.0010\n",
      "Epoch 79/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0251 - lr: 0.0010\n",
      "Epoch 80/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0269 - lr: 0.0010\n",
      "Epoch 81/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0221 - lr: 0.0010\n",
      "Epoch 82/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0266 - lr: 0.0010\n",
      "Epoch 83/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0242 - lr: 0.0010\n",
      "Epoch 84/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0231 - lr: 0.0010\n",
      "Epoch 85/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0265 - lr: 0.0010\n",
      "Epoch 86/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0283 - lr: 0.0010\n",
      "Epoch 87/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0266 - lr: 0.0010\n",
      "Epoch 88/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0316 - lr: 0.0010\n",
      "Epoch 89/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0231 - lr: 0.0010\n",
      "Epoch 90/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0297 - lr: 0.0010\n",
      "Epoch 91/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0253 - lr: 0.0010\n",
      "Epoch 92/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0345 - lr: 0.0010\n",
      "Epoch 93/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0239 - lr: 0.0010\n",
      "Epoch 94/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0256 - lr: 0.0010\n",
      "Epoch 95/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0311 - lr: 0.0010\n",
      "Epoch 96/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0274 - lr: 0.0010\n",
      "Epoch 97/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0233 - lr: 0.0010\n",
      "Epoch 98/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0289 - lr: 0.0010\n",
      "Epoch 99/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0254 - lr: 0.0010\n",
      "Epoch 100/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0248 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f797cc35050>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the LSTM model\n",
    "lstm_model = Sequential()\n",
    "# lstm_model.add(LSTM(64, input_shape=(look_back, train_X.shape[1])))\n",
    "lstm_model.add(LSTM(32, input_shape=(look_back, train_X.shape[1]), return_sequences=True))\n",
    "lstm_model.add(LSTM(16))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "# Define the callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "# Train the LSTM model with the callback\n",
    "batch_size = 8\n",
    "lstm_model.fit(trainX, trainY, epochs=100, batch_size=batch_size, verbose=1, validation_data=(testX, testY), callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f797d2e0950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f797d2e0950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1025WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f7991916830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f7991916830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "24/24 [==============================] - 3s 32ms/step - loss: 0.0885 - val_loss: 0.0262 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0076 - val_loss: 0.0216 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0203 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0204 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0211 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0199 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0221 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0196 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0201 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0212 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0228 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0198 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0197 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0219 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0215 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0203 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0238 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0217 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0196 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0195 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0196 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0220 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0224 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0197 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0240 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0197 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0212 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0213 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0236 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0248 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0206 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0201 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0267 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0234 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0207 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0209 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0226 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0231 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0236 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0192 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0210 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0190 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0190 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0191 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0203 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0205 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0270 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0210 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0212 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0253 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0242 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0272 - lr: 0.0010\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0224 - lr: 0.0010\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0196 - lr: 0.0010\n",
      "Epoch 55/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0197 - lr: 0.0010\n",
      "Epoch 56/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0212 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0194 - lr: 0.0010\n",
      "Epoch 58/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0190 - lr: 0.0010\n",
      "Epoch 59/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0187 - lr: 0.0010\n",
      "Epoch 60/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0189 - lr: 0.0010\n",
      "Epoch 61/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0213 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0214 - lr: 0.0010\n",
      "Epoch 63/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0205 - lr: 0.0010\n",
      "Epoch 64/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0197 - lr: 0.0010\n",
      "Epoch 65/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0214 - lr: 0.0010\n",
      "Epoch 66/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0211 - lr: 0.0010\n",
      "Epoch 67/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0235 - lr: 0.0010\n",
      "Epoch 68/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0189 - lr: 0.0010\n",
      "Epoch 69/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0189 - lr: 0.0010\n",
      "Epoch 70/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0089 - val_loss: 0.0197 - lr: 0.0010\n",
      "Epoch 71/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0261 - lr: 0.0010\n",
      "Epoch 72/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0245 - lr: 0.0010\n",
      "Epoch 73/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0254 - lr: 0.0010\n",
      "Epoch 74/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0211 - lr: 0.0010\n",
      "Epoch 75/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0207 - lr: 0.0010\n",
      "Epoch 76/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0228 - lr: 0.0010\n",
      "Epoch 77/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0227 - lr: 0.0010\n",
      "Epoch 78/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0202 - lr: 0.0010\n",
      "Epoch 79/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0196 - lr: 0.0010\n",
      "Epoch 80/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0220 - lr: 0.0010\n",
      "Epoch 81/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0200 - lr: 0.0010\n",
      "Epoch 82/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0190 - lr: 0.0010\n",
      "Epoch 83/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0271 - lr: 0.0010\n",
      "Epoch 84/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0201 - lr: 0.0010\n",
      "Epoch 85/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0192 - lr: 0.0010\n",
      "Epoch 86/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0190 - lr: 0.0010\n",
      "Epoch 87/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0235 - lr: 0.0010\n",
      "Epoch 88/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0258 - lr: 0.0010\n",
      "Epoch 89/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0209 - lr: 0.0010\n",
      "Epoch 90/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0208 - lr: 0.0010\n",
      "Epoch 91/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0222 - lr: 0.0010\n",
      "Epoch 92/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0196 - lr: 0.0010\n",
      "Epoch 93/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0199 - lr: 0.0010\n",
      "Epoch 94/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0214 - lr: 0.0010\n",
      "Epoch 95/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0189 - lr: 0.0010\n",
      "Epoch 96/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0260 - lr: 0.0010\n",
      "Epoch 97/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0223 - lr: 0.0010\n",
      "Epoch 98/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0221 - lr: 0.0010\n",
      "Epoch 99/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0236 - lr: 0.0010\n",
      "Epoch 100/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0206 - lr: 0.0010\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f7993a8b830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f7993a8b830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f797b20ed40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f797b20ed40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the GRU model\n",
    "# Build the GRU model\n",
    "gru_model = Sequential()\n",
    "gru_model.add(GRU(32, input_shape=(look_back, train_X.shape[1]), return_sequences=True))\n",
    "gru_model.add(GRU(16))\n",
    "gru_model.add(Dense(1))\n",
    "gru_model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "# Train the GRU model with the callback\n",
    "gru_model.fit(trainX, trainY, epochs=100, batch_size=batch_size, verbose=1, validation_data=(testX, testY), callbacks=[reduce_lr])\n",
    "\n",
    "# Make predictions\n",
    "train_lstm_predict = lstm_model.predict(trainX)\n",
    "test_lstm_predict = lstm_model.predict(testX)\n",
    "train_gru_predict = gru_model.predict(trainX)\n",
    "test_gru_predict = gru_model.predict(testX)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "train_lstm_predict = scaler.inverse_transform(train_lstm_predict)\n",
    "test_lstm_predict = scaler.inverse_transform(test_lstm_predict)\n",
    "train_gru_predict = scaler.inverse_transform(train_gru_predict)\n",
    "test_gru_predict = scaler.inverse_transform(test_gru_predict)\n",
    "trainY = scaler.inverse_transform(trainY.reshape(-1, 1))\n",
    "testY = scaler.inverse_transform(testY.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í‰ê°€ì§€í‘œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Train RMSE: 2220.34\n",
      "LSTM Test RMSE: 7746.21\n",
      "GRU Train RMSE: 2034.06\n",
      "GRU Test RMSE: 7071.03\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Calculate the RMSE and MAE for the LSTM model\n",
    "train_lstm_rmse = np.sqrt(mean_squared_error(trainY, train_lstm_predict))\n",
    "test_lstm_rmse = np.sqrt(mean_squared_error(testY, test_lstm_predict))\n",
    "train_lstm_mae = mean_absolute_error(trainY, train_lstm_predict)\n",
    "test_lstm_mae = mean_absolute_error(testY, test_lstm_predict)\n",
    "\n",
    "print('LSTM Train RMSE: %.2f' % train_lstm_rmse)\n",
    "print('LSTM Test RMSE: %.2f' % test_lstm_rmse)\n",
    "# print('LSTM Train MAE: %.2f' % train_lstm_mae)\n",
    "# print('LSTM Test MAE: %.2f' % test_lstm_mae)\n",
    "\n",
    "# Calculate the RMSE and MAE for the GRU model\n",
    "train_gru_rmse = np.sqrt(mean_squared_error(trainY, train_gru_predict))\n",
    "test_gru_rmse = np.sqrt(mean_squared_error(testY, test_gru_predict))\n",
    "train_gru_mae = mean_absolute_error(trainY, train_gru_predict)\n",
    "test_gru_mae = mean_absolute_error(testY, test_gru_predict)\n",
    "\n",
    "print('GRU Train RMSE: %.2f' % train_gru_rmse)\n",
    "print('GRU Test RMSE: %.2f' % test_gru_rmse)\n",
    "# print('GRU Train MAE: %.2f' % train_gru_mae)\n",
    "# print('GRU Test MAE: %.2f' % test_gru_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ff8cbd4a2e901bf9e1e261a76cbdb5a566aebd8efe282e332c66c7c4471c704"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
